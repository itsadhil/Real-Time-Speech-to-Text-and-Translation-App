{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "speech_recognition_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\", model=\"openai/whisper-base\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cmu-arctic-xvectors (/Users/kensonhui/.cache/huggingface/datasets/Matthijs___cmu-arctic-xvectors/default/0.0.1/a62fea1f9415e240301ea0042ffad2a3aadf4d1caa7f9a8d9512d631723e781f)\n"
     ]
    }
   ],
   "source": [
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load voice syntheizer model\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "\n",
    "# GPU device if we have one\n",
    "model.to(device)\n",
    "vocoder.to(device)\n",
    "\n",
    "# Load speaker embeddings\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from transformers.pipelines.audio_utils import ffmpeg_microphone_live\n",
    "\n",
    "target_dtype = np.int16\n",
    "max_range = np.iinfo(target_dtype).max\n",
    "\n",
    "def synthesise(text):\n",
    "    inputs = processor(text=text, return_tensors=\"pt\")\n",
    "    speech = model.generate_speech(\n",
    "        inputs[\"input_ids\"].to(device), speaker_embeddings.to(device), vocoder=vocoder\n",
    "    )\n",
    "    return speech.cpu()\n",
    "\n",
    "def translate(audio):\n",
    "    outputs = speech_recognition_pipe(audio, max_new_tokens=256, generate_kwargs={\"task\": \"translate\"})\n",
    "    return outputs[\"text\"]\n",
    "\n",
    "def speech_to_speech_translation(audio):\n",
    "    translated_text = translate(audio)\n",
    "    synthesised_speech = synthesise(translated_text)\n",
    "    synthesised_speech = (synthesised_speech.numpy() * max_range).astype(np.int16)\n",
    "    return 16000, synthesised_speech\n",
    "\n",
    "def transcribe(chunk_length_s=15.0, stream_chunk_s=1):\n",
    "    sampling_rate = speech_recognition_pipe.feature_extractor.sampling_rate\n",
    "\n",
    "    mic = ffmpeg_microphone_live(\n",
    "        sampling_rate=sampling_rate,\n",
    "        chunk_length_s=chunk_length_s,\n",
    "        stream_chunk_s=stream_chunk_s,\n",
    "    )\n",
    "\n",
    "    print(\"Start speaking...\")\n",
    "    for item in speech_recognition_pipe(mic, generate_kwargs={\"max_new_tokens\": 128, \"task\": \"translate\"}):\n",
    "        #sys.stdout.write(\"\\033[K\")\n",
    "        print(item[\"text\"], end=\"\\r\")\n",
    "        if not item[\"partial\"][0]:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start speaking...\n",
      " Do you want to try testing it again?\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/speech-to-speech/.conda/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:183\u001b[0m, in \u001b[0;36mPipelineChunkIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     \u001b[39m# Try to return next item\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     processed \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubiterator)\n\u001b[1;32m    184\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[39m# When a preprocess iterator ends, we can start lookig at the next item\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[39m# ChunkIterator will keep feeding until ALL elements of iterator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[39m# Another way to look at it, is we're basically flattening lists of lists\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[39m# into a single list, but with generators\u001b[39;00m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kensonhui/speech-to-speech/speech-to-transcribe.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kensonhui/speech-to-speech/speech-to-transcribe.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m transcribe(chunk_length_s\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n",
      "\u001b[1;32m/Users/kensonhui/speech-to-speech/speech-to-transcribe.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kensonhui/speech-to-speech/speech-to-transcribe.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m mic \u001b[39m=\u001b[39m ffmpeg_microphone_live(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kensonhui/speech-to-speech/speech-to-transcribe.ipynb#W3sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     sampling_rate\u001b[39m=\u001b[39msampling_rate,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kensonhui/speech-to-speech/speech-to-transcribe.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     chunk_length_s\u001b[39m=\u001b[39mchunk_length_s,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kensonhui/speech-to-speech/speech-to-transcribe.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     stream_chunk_s\u001b[39m=\u001b[39mstream_chunk_s,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kensonhui/speech-to-speech/speech-to-transcribe.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kensonhui/speech-to-speech/speech-to-transcribe.ipynb#W3sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStart speaking...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kensonhui/speech-to-speech/speech-to-transcribe.ipynb#W3sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m item \u001b[39min\u001b[39;49;00m speech_recognition_pipe(mic, generate_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mmax_new_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m128\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtask\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mtranslate\u001b[39;49m\u001b[39m\"\u001b[39;49m}):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kensonhui/speech-to-speech/speech-to-transcribe.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39m#sys.stdout.write(\"\\033[K\")\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kensonhui/speech-to-speech/speech-to-transcribe.ipynb#W3sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mprint\u001b[39;49m(item[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m], end\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\r\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kensonhui/speech-to-speech/speech-to-transcribe.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m item[\u001b[39m\"\u001b[39;49m\u001b[39mpartial\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m]:\n",
      "File \u001b[0;32m~/speech-to-speech/.conda/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/speech-to-speech/.conda/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:266\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[39mreturn\u001b[39;00m accumulator\n\u001b[1;32m    265\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 266\u001b[0m     processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(\u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    267\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed, torch\u001b[39m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/speech-to-speech/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/speech-to-speech/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/speech-to-speech/.conda/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     31\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m         data\u001b[39m.\u001b[39mappend(\u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_iter))\n\u001b[1;32m     33\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mended \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/speech-to-speech/.conda/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:191\u001b[0m, in \u001b[0;36mPipelineChunkIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     processed \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubiterator)\n\u001b[1;32m    184\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[39m# When a preprocess iterator ends, we can start lookig at the next item\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[39m# ChunkIterator will keep feeding until ALL elements of iterator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[39m# Another way to look at it, is we're basically flattening lists of lists\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[39m# into a single list, but with generators\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubiterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(\u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    192\u001b[0m     processed \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubiterator)\n\u001b[1;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m processed\n",
      "File \u001b[0;32m~/speech-to-speech/.conda/lib/python3.11/site-packages/transformers/pipelines/audio_utils.py:165\u001b[0m, in \u001b[0;36mffmpeg_microphone_live\u001b[0;34m(sampling_rate, chunk_length_s, stream_chunk_s, stride_length_s, format_for_conversion)\u001b[0m\n\u001b[1;32m    163\u001b[0m audio_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m    164\u001b[0m delta \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mtimedelta(seconds\u001b[39m=\u001b[39mchunk_s)\n\u001b[0;32m--> 165\u001b[0m \u001b[39mfor\u001b[39;49;00m item \u001b[39min\u001b[39;49;00m chunk_bytes_iter(microphone, chunk_len, stride\u001b[39m=\u001b[39;49m(stride_left, stride_right), stream\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m):\n\u001b[1;32m    166\u001b[0m     \u001b[39m# Put everything back in numpy scale\u001b[39;49;00m\n\u001b[1;32m    167\u001b[0m     item[\u001b[39m\"\u001b[39;49m\u001b[39mraw\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m=\u001b[39;49m np\u001b[39m.\u001b[39;49mfrombuffer(item[\u001b[39m\"\u001b[39;49m\u001b[39mraw\u001b[39;49m\u001b[39m\"\u001b[39;49m], dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    168\u001b[0m     item[\u001b[39m\"\u001b[39;49m\u001b[39mstride\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m=\u001b[39;49m (\n\u001b[1;32m    169\u001b[0m         item[\u001b[39m\"\u001b[39;49m\u001b[39mstride\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m] \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m size_of_sample,\n\u001b[1;32m    170\u001b[0m         item[\u001b[39m\"\u001b[39;49m\u001b[39mstride\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m1\u001b[39;49m] \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m size_of_sample,\n\u001b[1;32m    171\u001b[0m     )\n",
      "File \u001b[0;32m~/speech-to-speech/.conda/lib/python3.11/site-packages/transformers/pipelines/audio_utils.py:192\u001b[0m, in \u001b[0;36mchunk_bytes_iter\u001b[0;34m(iterator, chunk_len, stride, stream)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    189\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStride needs to be strictly smaller than chunk_len: (\u001b[39m\u001b[39m{\u001b[39;00mstride_left\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mstride_right\u001b[39m}\u001b[39;00m\u001b[39m) vs \u001b[39m\u001b[39m{\u001b[39;00mchunk_len\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m     )\n\u001b[1;32m    191\u001b[0m _stride_left \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 192\u001b[0m \u001b[39mfor\u001b[39;49;00m raw \u001b[39min\u001b[39;49;00m iterator:\n\u001b[1;32m    193\u001b[0m     acc \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m raw\n\u001b[1;32m    194\u001b[0m     \u001b[39mif\u001b[39;49;00m stream \u001b[39mand\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(acc) \u001b[39m<\u001b[39;49m chunk_len:\n",
      "File \u001b[0;32m~/speech-to-speech/.conda/lib/python3.11/site-packages/transformers/pipelines/audio_utils.py:98\u001b[0m, in \u001b[0;36mffmpeg_microphone\u001b[0;34m(sampling_rate, chunk_length_s, format_for_conversion)\u001b[0m\n\u001b[1;32m     96\u001b[0m chunk_len \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mround\u001b[39m(sampling_rate \u001b[39m*\u001b[39m chunk_length_s)) \u001b[39m*\u001b[39m size_of_sample\n\u001b[1;32m     97\u001b[0m iterator \u001b[39m=\u001b[39m _ffmpeg_stream(ffmpeg_command, chunk_len)\n\u001b[0;32m---> 98\u001b[0m \u001b[39mfor\u001b[39;49;00m item \u001b[39min\u001b[39;49;00m iterator:\n\u001b[1;32m     99\u001b[0m     \u001b[39myield\u001b[39;49;00m item\n",
      "File \u001b[0;32m~/speech-to-speech/.conda/lib/python3.11/site-packages/transformers/pipelines/audio_utils.py:223\u001b[0m, in \u001b[0;36m_ffmpeg_stream\u001b[0;34m(ffmpeg_command, buflen)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39mwith\u001b[39;00m subprocess\u001b[39m.\u001b[39mPopen(ffmpeg_command, stdout\u001b[39m=\u001b[39msubprocess\u001b[39m.\u001b[39mPIPE, bufsize\u001b[39m=\u001b[39mbufsize) \u001b[39mas\u001b[39;00m ffmpeg_process:\n\u001b[1;32m    222\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m         raw \u001b[39m=\u001b[39m ffmpeg_process\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mread(buflen)\n\u001b[1;32m    224\u001b[0m         \u001b[39mif\u001b[39;00m raw \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    225\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transcribe(chunk_length_s=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
